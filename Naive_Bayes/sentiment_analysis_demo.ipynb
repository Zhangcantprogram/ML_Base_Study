{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现过程\n",
    "\n",
    "# 1.获取数据\n",
    "# 2.数据基本处理\n",
    "#   2.1取出内容列，对数据进行分析\n",
    "#   2.2判定评判标准\n",
    "#   2.3选择停用词\n",
    "#   2.4把内容处理，转化成标准格式\n",
    "#   2.5统计词的个数\n",
    "#   2.6准备训练集和测试集\n",
    "# 3.模型训练\n",
    "# 4.模型评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "#!pip install jieba\n",
    "#### 顺便保存一下环境配置 O.o！ conda env export > D:\\Study\\VSCode\\vsCode\\ML\\environment.yml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "# 用于将文本转换为计数特征的工具，通常用于文本分类。\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 实现多项式朴素贝叶斯分类器的模块，适用于分类任务。\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>内容</th>\n",
       "      <th>评价</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>从编程小白的角度看，入门极佳。</td>\n",
       "      <td>好评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>很好的入门书，简洁全面，适合小白。</td>\n",
       "      <td>好评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>讲解全面，许多小细节都有顾及，三个小项目受益匪浅。</td>\n",
       "      <td>好评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>前半部分讲概念深入浅出，要言不烦，很赞</td>\n",
       "      <td>好评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>看了一遍还是不会写，有个概念而已</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>中规中矩的教科书，零基础的看了依旧看不懂</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>内容太浅显，个人认为不适合有其它语言编程基础的人</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>破书一本</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>适合完完全全的小白读，有其他语言经验的可以去看别的书</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>基础知识写的挺好的！</td>\n",
       "      <td>好评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>太基础</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>略_嗦。。适合完全没有编程经验的小白</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>真的真的不建议买</td>\n",
       "      <td>差评</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                          内容  评价\n",
       "0            0             从编程小白的角度看，入门极佳。  好评\n",
       "1            1           很好的入门书，简洁全面，适合小白。  好评\n",
       "2            2   讲解全面，许多小细节都有顾及，三个小项目受益匪浅。  好评\n",
       "3            3         前半部分讲概念深入浅出，要言不烦，很赞  好评\n",
       "4            4            看了一遍还是不会写，有个概念而已  差评\n",
       "5            5        中规中矩的教科书，零基础的看了依旧看不懂  差评\n",
       "6            6    内容太浅显，个人认为不适合有其它语言编程基础的人  差评\n",
       "7            7                        破书一本  差评\n",
       "8            8  适合完完全全的小白读，有其他语言经验的可以去看别的书  差评\n",
       "9            9                  基础知识写的挺好的！  好评\n",
       "10          10                         太基础  差评\n",
       "11          11          略_嗦。。适合完全没有编程经验的小白  差评\n",
       "12          12                    真的真的不建议买  差评"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv(\"./data/书籍评价.csv\", encoding=\"gbk\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x0000017C1A78D230>\n",
      "[' ', '从', '编程', '小白', '的', '角度看', '，', '入门', '极佳', '。']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D1C0>\n",
      "['很', '好', '的', '入门', '书', '，', '简洁', '全面', '，', '适合', '小白', '。']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D310>\n",
      "['讲解', '全面', '，', '许多', '小', '细节', '都', '有', '顾及', '，', '三个', '小', '项目', '受益匪浅', '。']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D380>\n",
      "['前半部', '分讲', '概念', '深入浅出', '，', '要言不烦', '，', '很赞']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D3F0>\n",
      "['看', '了', '一遍', '还是', '不会', '写', '，', '有个', '概念', '而已']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D690>\n",
      "['中规中矩', '的', '教科书', '，', '零', '基础', '的', '看', '了', '依旧', '看不懂']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D700>\n",
      "['内容', '太', '浅显', '，', '个人', '认为', '不', '适合', '有', '其它', '语言', '编程', '基础', '的', '人']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D770>\n",
      "['破书', '一本']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D850>\n",
      "['适合', '完完全全', '的', '小白读', '，', '有', '其他', '语言', '经验', '的', '可以', '去', '看', '别的', '书']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D8C0>\n",
      "['基础知识', '写', '的', '挺', '好', '的', '！']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78D930>\n",
      "['太', '基础']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78DA10>\n",
      "['略', '_', '嗦', '。', '。', '适合', '完全', '没有', '编程', '经验', '的', '小白']\n",
      "\n",
      "<generator object Tokenizer.cut at 0x0000017C1A78DA80>\n",
      "['真的', '真的', '不', '建议', '买']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Python\\Anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m con \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39mstopwords)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 进行词数统计\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 它通过 fit_transform 函数计算各个词语出现的次数\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[0;32m     49\u001b[0m name \u001b[38;5;241m=\u001b[39m con\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32md:\\Study\\Python\\Anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Study\\Python\\Anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Study\\Python\\Anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 数据基本处理\n",
    "# 2.1取出内容列，对数据进行分析\n",
    "content = data[\"内容\"]\n",
    "\n",
    "# 2.2 判定评判标准\n",
    "data.loc[data.loc[:,'评价'] == \"好评\", \"评论标号\"] = 1\n",
    "data.loc[data.loc[:,'评价'] == \"差评\", \"评论标号\"] = 0\n",
    "#print(data)\n",
    "\n",
    "good_or_bad = data['评价'].values\n",
    "\n",
    "# 2.3 选择停用词\n",
    "stopwords = []\n",
    "with open('./data/stopwords.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    #print(lines)\n",
    "    for tmp in lines:\n",
    "        line = tmp.strip()\n",
    "        #print(line)\n",
    "        stopwords.append(line)\n",
    "# stopwords\n",
    "\n",
    "# 对停用词进行去重\n",
    "stopwords = list(set(stopwords))\n",
    "\n",
    "# 2.4 把内容处理，转化为标准格式\n",
    "comment_list = []\n",
    "for tmp in content:\n",
    "    #print(tmp)\n",
    "    # 对文本数据进行切割\n",
    "    # cut_all 参数默认为False, 所有使用 cut 方法时默认为精确模式\n",
    "    # jieba.cut() 在调用时不会立即返回所有的分词结果，而是返回一个生成器对象，这里是 seg_list\n",
    "    seg_list = jieba.cut(tmp, cut_all=False)\n",
    "    print(seg_list)\n",
    "    print(list(seg_list))\n",
    "    seg_str = ','.join(seg_list)\n",
    "    print(seg_str)\n",
    "    comment_list.append(seg_str)\n",
    "#print(comment_list)\n",
    "\n",
    "# 2.5统计词个数\n",
    "# 实例化对象\n",
    "# CountVectorizer 类会将⽂本中的词语转换为词频矩阵\n",
    "con = CountVectorizer(stop_words=stopwords)\n",
    "# 进行词数统计\n",
    "# 它通过 fit_transform 函数计算各个词语出现的次数\n",
    "X = con.fit_transform(comment_list)\n",
    "print(X.toarray())\n",
    "name = con.get_feature_names_out()\n",
    "print(name)\n",
    "\n",
    "\n",
    "# 2.6）准备训练集和测试集\n",
    "# 准备训练集 这⾥将⽂本前10⾏当做训练集 后3⾏当做测试集\n",
    "x_train = X.toarray()[:10, :]\n",
    "y_train = good_or_bad[:10]\n",
    "# 准备测试集\n",
    "x_test = X.toarray()[10:, :]\n",
    "y_test = good_or_bad[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测值： ['差评' '差评' '差评']\n",
      "真实值： ['差评' '差评' '差评']\n"
     ]
    }
   ],
   "source": [
    "# 构建⻉叶斯算法分类器\n",
    "mb = MultinomialNB(alpha=1) # alpha 为可选项，默认 1.0，添加拉普拉修/Lidstone 平滑参\n",
    "# 训练数据\n",
    "mb.fit(x_train, y_train)\n",
    "# 预测数据\n",
    "y_predict = mb.predict(x_test)\n",
    "#预测值与真实值展示\n",
    "print('预测值：',y_predict)\n",
    "print('真实值：',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型评估\n",
    "mb.score(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
